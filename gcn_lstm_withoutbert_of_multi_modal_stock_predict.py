import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['KERAS_BACKEND']='tensorflow'
import tensorflow as tf
tf.config.list_physical_devices('GPU')
tf.debugging.set_log_device_placement(True)
physical_devices = tf.config.experimental.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)
# -*- coding: utf-8 -*-
# """GCN-LSTM of Multi_modal_Stock_Predict.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1YSeKedTROVZmSPZmzMS3HOc7I21tgc8L
# """

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

import os
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import date, timedelta
from scipy.stats import skew, kurtosis

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

import tensorflow as tf
import keras
from keras.models import Sequential, Model
from keras.layers import Dense, LSTM, Dropout, Input, Flatten, BatchNormalization, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K
import re

import tensorflow as tf

# from google.colab import drive
# drive.mount('/content/drive')

triker1 = 'AAPL'
# tt = ['ADBE', 'AMT', 'PLD', 'VICI', 'ATVI', 'JNJ', 'IBM', 'EBAY', 'GOOG']
tt = ['ADBE', 'PLD', 'BIO', 'JNJ', 'NVDA', 'IBM', 'NFLX', 'EBAY', 'AMZN', 'GOOG', 'TSLA']
for i in tt:
    triker1 = i
    triker = triker1.lower()


    trend = 'downtrend'
    # trend = 'uptrend'
    tmax_trials = 10
    tepochs = 5
    fepochs = 1

    # PATH = '/content/drive/MyDrive/stock_data/'
    # path = '/content/drive/MyDrive/stock_GCN_info/'
    PATH = '/home/ubuntu/GCN_FINBERT_LSTM/GCN_LSTM_STOCK_TREND_PREDICTION/stock_data/'
    path = '/home/ubuntu/GCN_FINBERT_LSTM/GCN_LSTM_STOCK_TREND_PREDICTION/GCN_STOCK_RESULT/stock_GCN_info/'

    ns = ['adbe', 'amt', 'pld', 'vici', 'schw', 'jpm', 'atvi', 'cvs', 'bio', 'jnj']

    def se(triker):
        if triker in ns:
            stock = pd.read_csv(PATH + triker + '_Simon/stocks_ts_'+triker1+'_2021-1-4_2022-9-20.csv')
            stock = stock[stock['date']>= '2021-01-01']
            stock = stock.iloc[:,0:25].drop(columns=['symbol', 'NextDayClose', 'NextDayTrend',	'PrevDayTrend'])
        else:
            stock = pd.read_csv(PATH + triker + '_Simon/stocks_ts_'+triker1+'_2021-1-4_2022-9-20.csv')
            stock = stock[stock['date']>= '2021-01-01']
            stock = stock.iloc[:,0:25].drop(columns=['symbol', 'NextDayClose', 'NextDayTrend',	'PrevDayTrend', 'splits'])
            return stock



    """# Load package 1"""

    import numpy as np
    from scipy import sparse as sp

    import tensorflow as tf
    from tensorflow.keras import backend as K
    from tensorflow.python.ops.linalg.sparse import sparse as tfsp
    from tensorflow.keras import backend as K

    SINGLE  = 1   # Single         (rank(a)=2, rank(b)=2)
    MIXED   = 2   # Mixed          (rank(a)=2, rank(b)=3)
    iMIXED  = 3   # Inverted mixed (rank(a)=3, rank(b)=2)
    BATCH   = 4   # Batch          (rank(a)=3, rank(b)=3)
    UNKNOWN = -1  # Unknown


    def transpose(a, perm=None, name=None):
        """
        Transposes a according to perm, dealing automatically with sparsity.
        :param a: Tensor or SparseTensor with rank k.
        :param perm: permutation indices of size k.
        :param name: name for the operation.
        :return: Tensor or SparseTensor with rank k.
        """
        if K.is_sparse(a):
            transpose_op = tf.sparse.transpose
        else:
            transpose_op = tf.transpose

        if perm is None:
            perm = (1, 0)  # Make explicit so that shape will always be preserved
        return transpose_op(a, perm=perm, name=name)


    def reshape(a, shape=None, name=None):
        """
        Reshapes a according to shape, dealing automatically with sparsity.
        :param a: Tensor or SparseTensor.
        :param shape: new shape.
        :param name: name for the operation.
        :return: Tensor or SparseTensor.
        """
        if K.is_sparse(a):
            reshape_op = tf.sparse.reshape
        else:
            reshape_op = tf.reshape

        return reshape_op(a, shape=shape, name=name)


    def autodetect_mode(a, b):
        """
        Return a code identifying the mode of operation (single, mixed, inverted mixed and
        batch), given a and b. See `ops.modes` for meaning of codes.
        :param a: Tensor or SparseTensor.
        :param b: Tensor or SparseTensor.
        :return: mode of operation as an integer code.
        """
        a_dim = K.ndim(a)
        b_dim = K.ndim(b)
        if b_dim == 2:
            if a_dim == 2:
                return SINGLE
            elif a_dim == 3:
                return iMIXED
        elif b_dim == 3:
            if a_dim == 2:
                return MIXED
            elif a_dim == 3:
                return BATCH
        return UNKNOWN


    def filter_dot(fltr, features):
        """
        Wrapper for matmul_A_B, specifically used to compute the matrix multiplication
        between a graph filter and node features.
        :param fltr:
        :param features: the node features (N x F in single mode, batch x N x F in
        mixed and batch mode).
        :return: the filtered features.
        """
        mode = autodetect_mode(fltr, features)
        if mode == SINGLE or mode == BATCH:
            return dot(fltr, features)
        else:
            # Mixed mode
            return mixed_mode_dot(fltr, features)


    def dot(a, b, transpose_a=False, transpose_b=False):
        """
        Dot product between a and b along innermost dimensions, for a and b with
        same rank. Supports both dense and sparse multiplication (including
        sparse-sparse).
        :param a: Tensor or SparseTensor with rank 2 or 3.
        :param b: Tensor or SparseTensor with same rank as a.
        :param transpose_a: bool, transpose innermost two dimensions of a.
        :param transpose_b: bool, transpose innermost two dimensions of b.
        :return: Tensor or SparseTensor with rank 2 or 3.
        """
        a_is_sparse_tensor = isinstance(a, tf.SparseTensor)
        b_is_sparse_tensor = isinstance(b, tf.SparseTensor)
        if a_is_sparse_tensor:
            a = tfsp.CSRSparseMatrix(a)
        if b_is_sparse_tensor:
            b = tfsp.CSRSparseMatrix(b)
        out = tfsp.matmul(a, b, transpose_a=transpose_a, transpose_b=transpose_b)
        if hasattr(out, 'to_sparse_tensor'):
            return out.to_sparse_tensor()

        return out


    def mixed_mode_dot(a, b):
        """
        Computes the equivalent of `tf.einsum('ij,bjk->bik', a, b)`, but
        works for both dense and sparse input filters.
        :param a: rank 2 Tensor or SparseTensor.
        :param b: rank 3 Tensor or SparseTensor.
        :return: rank 3 Tensor or SparseTensor.
        """
        s_0_, s_1_, s_2_ = K.int_shape(b)
        B_T = transpose(b, (1, 2, 0))
        B_T = reshape(B_T, (s_1_, -1))
        output = dot(a, B_T)
        output = reshape(output, (s_1_, s_2_, -1))
        output = transpose(output, (2, 0, 1))

        return output


    def degree_power(A, k):
        r"""
        Computes \(\D^{k}\) from the given adjacency matrix. Useful for computing
        normalised Laplacian.
        :param A: rank 2 array or sparse matrix.
        :param k: exponent to which elevate the degree matrix.
        :return: if A is a dense array, a dense array; if A is sparse, a sparse
        matrix in DIA format.
        """
        degrees = np.power(np.array(A.sum(1)), k).flatten()
        degrees[np.isinf(degrees)] = 0.
        if sp.issparse(A):
            D = sp.diags(degrees)
        else:
            D = np.diag(degrees)
        return D


    def normalized_adjacency(A, symmetric=True):
        r"""
        Normalizes the given adjacency matrix using the degree matrix as either
        \(\D^{-1}\A\) or \(\D^{-1/2}\A\D^{-1/2}\) (symmetric normalization).
        :param A: rank 2 array or sparse matrix;
        :param symmetric: boolean, compute symmetric normalization;
        :return: the normalized adjacency matrix.
        """
        if symmetric:
            normalized_D = degree_power(A, -0.5)
            output = normalized_D.dot(A).dot(normalized_D)
        else:
            normalized_D = degree_power(A, -1.)
            output = normalized_D.dot(A)
        return output


    def localpooling_filter(A, symmetric=True):
        r"""
        Computes the graph filter described in
        [Kipf & Welling (2017)](https://arxiv.org/abs/1609.02907).
        :param A: array or sparse matrix with rank 2 or 3;
        :param symmetric: boolean, whether to normalize the matrix as
        \(\D^{-\frac{1}{2}}\A\D^{-\frac{1}{2}}\) or as \(\D^{-1}\A\);
        :return: array or sparse matrix with rank 2 or 3, same as A;
        """
        fltr = A.copy()
        if sp.issparse(A):
            I = sp.eye(A.shape[-1], dtype=A.dtype)
        else:
            I = np.eye(A.shape[-1], dtype=A.dtype)
        if A.ndim == 3:
            for i in range(A.shape[0]):
                A_tilde = A[i] + I
                fltr[i] = normalized_adjacency(A_tilde, symmetric=symmetric)
        else:
            A_tilde = A + I
            fltr = normalized_adjacency(A_tilde, symmetric=symmetric)

        if sp.issparse(fltr):
            fltr.sort_indices()
        return fltr

    from tensorflow.keras import activations, initializers, regularizers, constraints
    from tensorflow.keras import backend as K
    from tensorflow.keras.layers import Layer



    class GraphConv(Layer):
        r"""
        A graph convolutional layer (GCN) as presented by
        [Kipf & Welling (2016)](https://arxiv.org/abs/1609.02907).
        **Mode**: single, mixed, batch.
        This layer computes:
        $$
            \Z = \hat \D^{-1/2} \hat \A \hat \D^{-1/2} \X \W + \b
        $$
        where \( \hat \A = \A + \I \) is the adjacency matrix with added self-loops
        and \(\hat\D\) is its degree matrix.
        **Input**
        - Node features of shape `([batch], N, F)`;
        - Modified Laplacian of shape `([batch], N, N)`; can be computed with
        `spektral.utils.convolution.localpooling_filter`.
        **Output**
        - Node features with the same shape as the input, but with the last
        dimension changed to `channels`.
        **Arguments**
        - `channels`: number of output channels;
        - `activation`: activation function to use;
        - `use_bias`: whether to add a bias to the linear transformation;
        - `kernel_initializer`: initializer for the kernel matrix;
        - `bias_initializer`: initializer for the bias vector;
        - `kernel_regularizer`: regularization applied to the kernel matrix;
        - `bias_regularizer`: regularization applied to the bias vector;
        - `activity_regularizer`: regularization applied to the output;
        - `kernel_constraint`: constraint applied to the kernel matrix;
        - `bias_constraint`: constraint applied to the bias vector.
        """

        def __init__(self,
                    channels,
                    activation=None,
                    use_bias=True,
                    kernel_initializer='glorot_uniform',
                    bias_initializer='zeros',
                    kernel_regularizer=None,
                    bias_regularizer=None,
                    activity_regularizer=None,
                    kernel_constraint=None,
                    bias_constraint=None,
                    **kwargs):

            super().__init__(activity_regularizer=activity_regularizer, **kwargs)
            self.channels = channels
            self.activation = activations.get(activation)
            self.use_bias = use_bias
            self.kernel_initializer = initializers.get(kernel_initializer)
            self.bias_initializer = initializers.get(bias_initializer)
            self.kernel_regularizer = regularizers.get(kernel_regularizer)
            self.bias_regularizer = regularizers.get(bias_regularizer)
            self.kernel_constraint = constraints.get(kernel_constraint)
            self.bias_constraint = constraints.get(bias_constraint)
            self.supports_masking = False

        def build(self, input_shape):
            assert len(input_shape) >= 2
            input_dim = input_shape[0][-1]
            self.kernel = self.add_weight(shape=(input_dim, self.channels),
                                        initializer=self.kernel_initializer,
                                        name='kernel',
                                        regularizer=self.kernel_regularizer,
                                        constraint=self.kernel_constraint)
            if self.use_bias:
                self.bias = self.add_weight(shape=(self.channels,),
                                            initializer=self.bias_initializer,
                                            name='bias',
                                            regularizer=self.bias_regularizer,
                                            constraint=self.bias_constraint)
            else:
                self.bias = None
            self.built = True

        def call(self, inputs):
            features = inputs[0]
            fltr = inputs[1]

            # Convolution
            output = dot(features, self.kernel)
            output = filter_dot(fltr, output)

            if self.use_bias:
                output = K.bias_add(output, self.bias)
            if self.activation is not None:
                output = self.activation(output)
            return output

        def compute_output_shape(self, input_shape):
            features_shape = input_shape[0]
            output_shape = features_shape[:-1] + (self.channels,)
            return output_shape

        def get_config(self):
            config = {
                'channels': self.channels,
                'activation': activations.serialize(self.activation),
                'use_bias': self.use_bias,
                'kernel_initializer': initializers.serialize(self.kernel_initializer),
                'bias_initializer': initializers.serialize(self.bias_initializer),
                'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
                'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                'kernel_constraint': constraints.serialize(self.kernel_constraint),
                'bias_constraint': constraints.serialize(self.bias_constraint)
            }
            base_config = super().get_config()
            return dict(list(base_config.items()) + list(config.items()))

        @staticmethod
        def preprocess(A):
            return localpooling_filter(A)

    """#Load data"""

    tt = ['ADBE', 'AMT', 'PLD', 'VICI', 'SCHW', 'JPM', 'ATVI', 'CVS', 'BIO', 'JNJ', 'NVDA', 'NDAQ', 'IBM', 'SBUX', 'NFLX', 'EBAY', 'AMZN', 'GOOG', 'AAPL', 'MSFT', 'TSLA']
    tt

    appended_data = []
    for ele in tt:
        triker_t = str(ele)
        # print(triker)
        triker2 = ele.lower()
        # print(triker2)
        stock = pd.read_csv(PATH + triker2 + '_Simon/stocks_ts_' + triker_t + '_2021-1-4_2022-9-20.csv')
        stock = stock[stock['date']>= '2021-01-04']
        stock = stock[stock['date']<= '2022-09-20']
        stock = stock[['date',	'symbol',	'close']]
        # store DataFrame in list
        appended_data.append(stock)
    # see pd.concat documentation for more info
    appended_data = pd.concat(appended_data)
    # write DataFrame to an excel sheet
    appended_data

    Multi_data1 = appended_data

    Multi_data1.shape

    """# Data visualization"""

    import datetime

    ### SWITCH DATA FROM VERTICAL TO HORIZONTAL FORMAT ###

    unstaked_df = Multi_data1.copy()
    # unstaked_df = unstaked_df.iloc[:,0:20]
    unstaked_df.set_index(['date','symbol'], inplace=True)
    unstaked_df = unstaked_df.astype(float).unstack().T
    print(unstaked_df.index[0][0])
    print(type(unstaked_df.index[0][0]))

    name = []
    symbol = []
    for i in unstaked_df.index:
        name.append(i[0])
        symbol.append(i[1])

    unstaked_df['id2'] = name
    unstaked_df['id1'] = symbol
    unstaked_df['id'] = unstaked_df['id1'].astype(str)+'_'+unstaked_df['id2'].astype(str)
    unstaked_df.set_index(['id'], inplace=True)
    unstaked_df.drop(['id2','id1'], axis=1, inplace=True)
    unstaked_df.columns = pd.to_datetime(unstaked_df.columns)
    print(type(unstaked_df.columns))

    print(unstaked_df.shape)
    unstaked_df.head()

        # idx = pd.date_range(start='2021-01-05', end='2022-09-20')
        # unstaked_df = unstaked_df.T.reindex(idx).T

        # unstaked_df.fillna(method='ffill', axis=1, inplace=True)


    unstaked_df = unstaked_df.sort_index(ascending=True)
    unstaked_df

    """# function for GCN model"""

    ### UTILITY FUNCTIONS FOR FEATURE ENGINEERING ###

    sequence_length = 14
    ns = 21
    sf = int(sequence_length/2)

    # sf = round(sequence_length/2)

    # def get_timespan(df, today, days):
    #     df = df[pd.date_range(today - timedelta(days=days),
    #             periods=days, freq='D')] # day - n_days <= dates < day
    #     return df


    def get_timespan(df, today, days):
        df = unstaked_df.iloc[:, unstaked_df.columns.get_loc(today)+1-days:unstaked_df.columns.get_loc(today)+1] # day - n_days <= dates < day
        return df

    #test for narmalization for close GCN price
    from sklearn.preprocessing import StandardScaler

    scaler = StandardScaler()

    unstaked_df.iloc[:,0:432] = scaler.fit_transform(unstaked_df.iloc[:,0:432])
    unstaked_df

    ### PLOT A SEQUENCE OF SALES FOR ITEM 10 IN ALL STORES ###

    sequence = get_timespan(unstaked_df, '2022-09-19', 30)
    sequence.head(18).T.plot(figsize=(14,5))
    plt.ylabel('sales')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

    """#create model"""

    ### TRAIN A MODEL FOR EACH STORES USING ALL THE DATA AVAILALBE FROM OTHER STORES ###
    """# Load Multimodal data"""

    tt[0]

    appended_data = []
    for ele in tt:
        triker_t = str(ele)
        # print(triker)
        triker2 = ele.lower()
        # print(triker1)
        stock = pd.read_csv(PATH + triker2 + '_Simon/stocks_ts_' + triker_t + '_2021-1-4_2022-9-20.csv')
        stock = stock[stock['date']>= '2021-01-01']
        stock = stock[stock['date']<= '2022-09-20']
        # store DataFrame in list
        appended_data.append(stock)
    # see pd.concat documentation for more info
    appended_data = pd.concat(appended_data)
    appended_data = appended_data.drop(['NextDayTrend',	'PrevDayTrend',	'dividends',	'splits'], axis = 1)
    # write DataFrame to an excel sheet
    appended_data

    stock = appended_data

    stock['Diff'] = stock['close'] - stock['PrevDayClose']
    stock['Diff'] = stock['Diff'].apply(lambda x: 1 if x > 0 else 0)
    stock.head(3)

    news = pd.read_csv(PATH +triker+'_Simon/Single_newsheadline_day_'+triker+'.csv')
    news = news[['date', 'Titles']]
    news

    Single_newsheadline_day_stock = news.merge(stock, on = 'date')
    Single_newsheadline_day_stock

    Multi_data = Single_newsheadline_day_stock

    # drop NA
    Multi_data = Multi_data.dropna()
    Multi_data = Multi_data.drop(columns=['PrevDayClose'])
    Multi_data

    Multi_data.iloc[:,2:19]

    from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    Multi_data.iloc[:,3:20] = scaler.fit_transform(Multi_data.iloc[:,3:20])
    Multi_data = Multi_data.sort_values(by=["symbol", "date"])
    Multi_data

    """#correlation calculator"""
    
    # 全局變量用於存儲固定的 GCN 相關性矩陣和特徵
    fixed_corr = None
    fixed_features = None
    use_fixed_gcn = True  # 設為 True 使用固定值，False 使用動態更新

    def create_features(df, today):
        global fixed_corr, fixed_features, use_fixed_gcn
        
        if use_fixed_gcn and fixed_corr is not None and fixed_features is not None:
            # 返回固定的相關性矩陣和特徵
            print("使用固定的 GCN 相關性矩陣和特徵")
            return fixed_corr, fixed_features

        td = unstaked_df.iloc[:, 0:unstaked_df.columns.get_loc(today)+1]

        all_sequence = td.values

        group_store = all_sequence.reshape((1, ns, -1))

        store_corr = np.stack([np.corrcoef(i) for i in group_store], axis=0)

        store_features = np.stack([
                group_store.mean(axis=2),
                group_store[:,:,1:].mean(axis=2),
                group_store.std(axis=2),
                group_store[:,:,1:].std(axis=2),
                skew(group_store, axis=2),
                kurtosis(group_store, axis=2),
                np.apply_along_axis(lambda x: np.polyfit(np.arange(0, group_store.shape[-1]), x, 1)[0], 2, group_store)
                ], axis=1)

        group_store = np.transpose(group_store, (0,2,1))

        store_features = np.transpose(store_features, (0,2,1))
        
        # 如果是第一次計算且啟用固定模式，儲存為固定值
        if use_fixed_gcn and fixed_corr is None:
            fixed_corr = store_corr.copy()
            fixed_features = store_features.copy()
            print("固定相關性矩陣和特徵已設定，形狀：", fixed_corr.shape, fixed_features.shape)

        return store_corr, store_features

    corr_, feat_ = create_features(unstaked_df, '2022-09-13')

    unstaked_df.iloc[:, 0:unstaked_df.columns.get_loc('2022-09-13')+1].values.shape

    A = unstaked_df.iloc[:, 0:unstaked_df.columns.get_loc('2022-09-13')+1].values.reshape((1, ns, -1))
    A

    A.shape

    np.stack([np.corrcoef(i) for i in A], axis=0)[0].shape

    corr_

    """# Text Cleaning

    Generating word frequencies
    """

    def gen_freq(text):
        #will store all the words in list
        words_list = []

        #Loop over all the words and extract word from list
        for word in text.split():
            words_list.extend(word)

        #Generate word frequencies using value counts in word_list
        word_freq = pd.Series(words_list).value_counts()

        #print top 100 words
        word_freq[:100]

        return word_freq

    freq = gen_freq(Multi_data.Titles.str)
    freq

    """Removing Stopwords"""

    import nltk
    nltk.download('stopwords')
    from nltk.corpus import stopwords
    stop_word_list = stopwords.words('english')

    from nltk.tokenize import word_tokenize,sent_tokenize
    from nltk.tokenize.toktok import ToktokTokenizer

    #Tokenization of text
    tokenizer=ToktokTokenizer()

    #removing the stopwords
    def remove_stopwords(text, is_lower_case=False):
        tokens = tokenizer.tokenize(text)
        tokens = [token.strip() for token in tokens]
        if is_lower_case:
            filtered_tokens = [token for token in tokens if token not in stop_word_list]
        else:
            filtered_tokens = [token for token in tokens if token.lower() not in stop_word_list]
        filtered_text = ' '.join(filtered_tokens)
        return filtered_text

    #Apply function on review column
    Multi_data['Titles']= Multi_data['Titles'].apply(remove_stopwords)

    """process of clearing punctuation marks in data
    cleaning unnecessary marks in data. </p> </li>
    capitalization to lowercase. </p> </li>
    cleaning extra spaces. </p> </li>
    removal of stopwords in sentences. </p> </li>
    """

    import re
    #clearing punctuation & unnecessary marks
    Multi_data['Titles']= Multi_data['Titles'].apply(lambda x: re.sub("[,’\.!?:-;()–']", '', x))
    # Multi_data['Titles']= Multi_data['Titles'].apply(lambda x: re.sub('[^a-zA-Z"]', ' ', x))

    #capitalization to lowercase
    Multi_data['Titles']= Multi_data['Titles'].apply(lambda x: x.lower())

    #cleaning extra spaces
    Multi_data['Titles']= Multi_data['Titles'].apply(lambda x: x.strip())

    #Removing the square brackets
    # def remove_between_square_brackets(text):
    #     return re.sub('\[[^]]*\]', '', text)

    #Apply function on review column

    # Multi_data['Titles']= Multi_data['Titles'].apply(remove_between_square_brackets)



    """# Early fusion Multimodal

    LSTM model
    """

    from keras.models import Sequential, Model
    from keras.layers import Dense, LSTM, Dropout, Bidirectional
    from keras.layers import LSTM

    # lstm_model = Sequential();
    # lstm_model.add(LSTM(500, return_sequences=True, input_shape=(None,Multi_data.shape[1]-4)));
    # lstm_model.add(tf.keras.layers.Normalization());
    # lstm_model.add(LSTM(450, dropout=0.1));
    # lstm_model.add(tf.keras.layers.BatchNormalization());
    # lstm_model.add(Dense(72))

    lstm_model = Sequential();
    lstm_model.add(LSTM(500, return_sequences=True, input_shape=(None,Multi_data.shape[1]-4)));
    lstm_model.add(Bidirectional(LSTM(128, dropout=0.8)));
    lstm_model.add(Dense(72))

    lstm_model = Model(lstm_model.inputs, lstm_model.outputs)

    lstm_model.summary()

    # from tensorflow.keras.utils import plot_model
    # plot_model(lstm_model)

    """GCN model"""

    ns = 21

    # inp_lap = Input((ns, ns))
    # inp_feat = Input((ns, sf))

    inp_lap = Input((ns, ns))
    inp_feat = Input((ns, sf))

    x = GraphConv(32, activation='relu')([inp_feat, inp_lap])
    x = GraphConv(16, activation='relu')([x, inp_lap])
    x = Flatten()(x)


    out = Dense(72)(x)

    gcn_model = Model([inp_lap, inp_feat], out)

    gcn_model.summary()

    # plot_model(gcn_model)



    """multimodal"""

    from tensorflow.keras import models
    # Stacking early-fusion multimodal model
    #nClasses = 2 # for multi-class

    lstm_input = tf.keras.layers.Input(shape=(None, Multi_data.shape[1]-4), dtype=tf.float32, name="Price")

    gcn_cor_input = tf.keras.layers.Input(shape=(inp_lap.shape[1],inp_lap.shape[2]), dtype=tf.float32, name="cor")
    gcn_sf_input = tf.keras.layers.Input(shape=(inp_feat.shape[1],inp_feat.shape[2]), dtype=tf.float32, name="stat_feat")


    lstm_side = lstm_model(lstm_input)

    gcn_side = gcn_model([gcn_cor_input, gcn_sf_input])

    # Concatenate features from LSTM and GCN only (without BERT)
    merged = tf.keras.layers.Concatenate()([lstm_side, gcn_side])
    merged = tf.keras.layers.Normalization()(merged)
    merged = tf.keras.layers.Dense(40, activation="relu")(merged)
    merged = tf.keras.layers.Normalization()(merged)
    merged = tf.keras.layers.Dense(60, activation="relu")(merged)
    output = tf.keras.layers.Dense(1, activation="sigmoid", name="class")(merged)

    merge_model = models.Model([lstm_input, gcn_cor_input, gcn_sf_input], output)

    merge_model.summary()

    # plot_model(merge_model)

    """# Model Compile"""

    #import mlflow.tensorflow
    from tensorflow.keras.optimizers import Adam

    # optimizer and metric
    merge_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"]) # default lr = 0.001
    # n_epochs = 10

    # train_data.element_spec

    """# Early stopping and model saving"""

    # checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)
    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1) # patience try 5 or 10

    # Save and Load model
    checkpoint_filepath = '/content/drive/MyDrive/aapl_stock_early_fusion_tuning.h5'
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        save_weights_only=True,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    """# Exponential Scheduling"""

    def exponential_decay_fn(epoch):
        return 0.01 * 0.1**(epoch / 12.5)

    def exponential_decay(lr0, s):
        def exponential_decay_fn(epoch):
            return lr0 * 0.1**(epoch / s)
        return exponential_decay_fn

    exponential_decay_fn = exponential_decay(lr0=0.01, s=12.5)

    import keras

    # callbacks=[lr_scheduler]
    lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)

    """# Training model

    # 10 days in training set 1 day for testing, grad search for hyper parameter tuning, evaluate in next year.

    # Function for loop
    """

    DATA_COLUMN = 'Titles'
    LABEL_COLUMN = 'Diff'

    len(Multi_data)



    def dc(d):
        d = d.to_string().split(' ')[-1]
        a = int(d.split('-')[0])
        b = int(d.split('-')[1])
        c = int(d.split('-')[2])
        return date(a,b,c)

    # Create a dataset from the generator function
    def train_matrix_to_tfdata(data):
        # Define generator function
        def generator(data):
            for i in range(data.shape[0]):
                yield data[i]

        traindata = tf.data.Dataset.from_generator(lambda: generator(data), output_signature=tf.TensorSpec(shape=(ns, ns), dtype=tf.float32))

        # Batch and reshape the dataset
        traindata = traindata.batch(batch_size)
        traindata = traindata.map(lambda x: tf.reshape(x, (batch_size, ns, ns)))

        return traindata

    # Create a dataset from the generator function
    def test_matrix_to_tfdata(data):
        # Define generator function
        def generator(data):
            for i in range(data.shape[0]):
                yield data[i]

        testdata = tf.data.Dataset.from_generator(lambda: generator(data), output_signature=tf.TensorSpec(shape=(ns, ns), dtype=tf.float32))

        # Batch and reshape the dataset
        testdata = testdata.batch(batch_size)
        testdata = testdata.map(lambda x: tf.reshape(x, (batch_size, ns, ns)))

        return testdata

    # Create a dataset from the generator function
    def train_matrix_to_tfdatafeat(data):
        # Define generator function
        def generator(data):
            for i in range(data.shape[0]):
                yield data[i]

        trainfeatdata = tf.data.Dataset.from_generator(lambda: generator(data), output_signature=tf.TensorSpec(shape=(ns, sf), dtype=tf.float32))

        # Batch and reshape the dataset
        trainfeatdata = trainfeatdata.batch(batch_size)
        trainfeatdata = trainfeatdata.map(lambda x: tf.reshape(x, (batch_size, ns, sf))) # time_steps

        return trainfeatdata

    # Create a dataset from the generator function
    def test_matrix_to_tfdatafeat(data):
        # Define generator function
        def generator(data):
            for i in range(data.shape[0]):
                yield data[i]

        testfeatdata = tf.data.Dataset.from_generator(lambda: generator(data), output_signature=tf.TensorSpec(shape=(ns, sf), dtype=tf.float32))

        # Batch and reshape the dataset
        testfeatdata = testfeatdata.batch(batch_size)
        testfeatdata = testfeatdata.map(lambda x: tf.reshape(x, (batch_size, ns, sf)))

        return testfeatdata

    def create_features(df, today):
        global fixed_corr, fixed_features, use_fixed_gcn
        
        if use_fixed_gcn and fixed_corr is not None and fixed_features is not None:
            # 返回固定的相關性矩陣和特徵
            return fixed_corr, fixed_features

        td = unstaked_df.iloc[:, 0:unstaked_df.columns.get_loc(today)+1]

        all_sequence = td.values

        group_store = all_sequence.reshape((1, ns, -1))

        store_corr = np.stack([np.corrcoef(i) for i in group_store], axis=0)

        store_features = np.stack([
                group_store.mean(axis=2),
                group_store[:,:,1:].mean(axis=2),
                group_store.std(axis=2),
                group_store[:,:,1:].std(axis=2),
                skew(group_store, axis=2),
                kurtosis(group_store, axis=2),
                np.apply_along_axis(lambda x: np.polyfit(np.arange(0, group_store.shape[-1]), x, 1)[0], 2, group_store)
                ], axis=1)

        group_store = np.transpose(group_store, (0,2,1))

        store_features = np.transpose(store_features, (0,2,1))
        
        # 如果是第一次計算且啟用固定模式，儲存為固定值
        if use_fixed_gcn and fixed_corr is None:
            fixed_corr = store_corr.copy()
            fixed_features = store_features.copy()

        return store_corr, store_features

    """# First model"""

    training = Multi_data.iloc[0:11]
    testing = Multi_data[11:13]
    testing

    # Accumulative price from the beggining of the day and evaluate to traning set
    li = []
    train = training
    feature_traindata = train.iloc[:,3:20].to_numpy()
    label_train = train.Diff.values

    test = testing
    feature_testdata = test.iloc[:,3:20].to_numpy()
    label_test = test.Diff.values

    time_steps = 10
    batch_size = 1

    train_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
        data = feature_traindata,
        targets = label_train,
        length = time_steps,
        sampling_rate=1,
        stride=1,
        start_index=0,
        end_index=None,
        shuffle=False,
        reverse=False,
        batch_size=batch_size
    )
    test_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
        data = feature_testdata,
        targets = label_test,
        length = 1,
        sampling_rate=1,
        stride=1,
        start_index=0,
        end_index=None,
        shuffle=False,
        reverse=False,
        batch_size=batch_size
    )


    ###############################################################################################################
    ### CREATE TRAIN FEATURES ###
    # For LSTM

    train_head = train.head(1)['date'].to_string().split()[-1]
    train_tail = train.tail(1)['date'].to_string().split()[-1]
    test_head = test.head(1)['date'].to_string().split()[-1]
    test_tail = test.tail(1)['date'].to_string().split()[-1]

    X_seq, X_cor, X_feat = [], [], []

    for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(train_head):unstaked_df.columns.get_loc(train_tail)]:
        corr_, feat_ = create_features(unstaked_df, d)
        X_cor.append(corr_), X_feat.append(feat_)

    X_train_cor = np.concatenate(X_cor, axis=0).astype('float16')
    X_train_feat = np.concatenate(X_feat, axis=0).astype('float16')

    print(X_train_cor.shape, X_train_feat.shape)

    # ## CREATE TEST FEATURES ###

    X_seq, X_cor, X_feat = [], [], []

    for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(test_head):unstaked_df.columns.get_loc(test_tail)+1]:
        corr_, feat_ = create_features(unstaked_df, d)
        X_cor.append(corr_), X_feat.append(feat_)

    X_test_cor = np.concatenate(X_cor, axis=0).astype('float16')
    X_test_feat = np.concatenate(X_feat, axis=0).astype('float16')

    print(X_test_cor.shape, X_test_feat.shape)

    ### SCALE SEQUENCES ###

    scaler_seq = StandardScaler()
    scaler_feat = StandardScaler()

    X_train_feat = scaler_feat.fit_transform(X_train_feat.reshape(-1,ns)).reshape(X_train_feat.shape)
    X_test_feat = scaler_feat.transform(X_test_feat.reshape(-1,ns)).reshape(X_test_feat.shape)

    ### OBTAIN LAPLACIANS FROM CORRELATIONS ###

    X_train_lap = localpooling_filter(1 - np.abs(X_train_cor))
    X_test_lap = localpooling_filter(1 - np.abs(X_test_cor))

    stock_cor_train_data = train_matrix_to_tfdata(X_train_lap)
    stock_cor_test_data = test_matrix_to_tfdata(X_test_lap)

    stock_feat_train_data = train_matrix_to_tfdatafeat(X_train_feat)
    stock_feat_test_data = test_matrix_to_tfdatafeat(X_test_feat)
    #################################################################################################################

    stock_lstm_train_data = tf.data.Dataset.from_generator(
        lambda: train_generator,
        output_types=(tf.float32, tf.float32),
        output_shapes=([batch_size, time_steps, feature_traindata.shape[1]], [batch_size,]) # based on batch_size change
    )

    stock_lstm_test_data = tf.data.Dataset.from_generator(
        lambda: test_generator,
        output_types=(tf.float32, tf.float32),
        output_shapes=([batch_size, 1, feature_testdata.shape[1]], [batch_size,]) # based on batch_size change
    )

    # price test batch drop remainder
    stock_lstm_test_data = stock_lstm_test_data.unbatch()
    stock_lstm_test_data = stock_lstm_test_data.batch(batch_size, drop_remainder=True)


    # merged train data without BERT
    try_data = tf.data.Dataset.zip((stock_lstm_train_data, stock_feat_train_data, stock_cor_train_data))
    try_data.element_spec
    train_data = try_data.map(lambda x, x1, x2: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0)), x[1]))
    print(train_data.element_spec)

    # merged validation data without BERT
    try_data2 = tf.data.Dataset.zip((stock_lstm_test_data, stock_feat_test_data, stock_cor_test_data))
    try_data2.element_spec
    test_data = try_data2.map(lambda x, x1, x2: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0)), x[1]))
    print(test_data.element_spec)

    # Train the model
    earlyfs_history = merge_model.fit(train_data,
                                        epochs=2, # 20
                                        callbacks=[lr_scheduler],
                                        # verbose=1
    )

    eval_results = merge_model.predict(test_data)
    print(eval_results)
    li.append(eval_results)

    train_data.element_spec

    test_data.element_spec

    """# Tuning data"""

    # # train = Multi_data[0:11]
    # # feature_traindata = train.iloc[:,3:20].to_numpy()
    # # label_train = train.Diff.values

    # # test = Multi_data[9:10+1]
    # # feature_testdata = test.iloc[:,3:20].to_numpy()
    # # label_test = test.Diff.values

    # # time_steps = 10
    # # batch_size = 1

    # train = Multi_data[0:21]
    # feature_traindata = train.iloc[:,3:20].to_numpy()
    # label_train = train.Diff.values

    # test = Multi_data[19:20+1]
    # feature_testdata = test.iloc[:,3:20].to_numpy()
    # label_test = test.Diff.values

    # time_steps = 20
    # batch_size = 1

    # train_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
    #       data = feature_traindata,
    #       targets = label_train,
    #       length = time_steps,
    #       sampling_rate=1,
    #       stride=1,
    #       start_index=0,
    #       end_index=None,
    #       shuffle=False,
    #       reverse=False,
    #       batch_size=batch_size
    #   )
    # test_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
    #       data = feature_testdata,
    #       targets = label_test,
    #       length = 1,
    #       sampling_rate=1,
    #       stride=1,
    #       start_index=0,
    #       end_index=None,
    #       shuffle=False,
    #       reverse=False,
    #       batch_size=batch_size
    #   )
    # #################################################################################################################
    # train_head = train.head(1)['date'].to_string().split()[-1]
    # train_tail = train.tail(1)['date'].to_string().split()[-1]
    # test_head = test.head(1)['date'].to_string().split()[-1]
    # test_tail = test.tail(1)['date'].to_string().split()[-1]

    # X_seq, X_cor, X_feat = [], [], []

    # for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(train_head):unstaked_df.columns.get_loc(train_tail)]:
    #     corr_, feat_ = create_features(unstaked_df, d)
    #     X_cor.append(corr_), X_feat.append(feat_)

    # X_train_cor = np.concatenate(X_cor, axis=0).astype('float16')
    # X_train_feat = np.concatenate(X_feat, axis=0).astype('float16')

    # print(X_train_cor.shape, X_train_feat.shape)

    # # ## CREATE TEST FEATURES ###

    # X_seq, X_cor, X_feat = [], [], []

    # for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(test_head):unstaked_df.columns.get_loc(test_tail)+1]:
    #     corr_, feat_ = create_features(unstaked_df, d)
    #     X_cor.append(corr_), X_feat.append(feat_)

    # X_test_cor = np.concatenate(X_cor, axis=0).astype('float16')
    # X_test_feat = np.concatenate(X_feat, axis=0).astype('float16')

    # print(X_test_cor.shape, X_test_feat.shape)

    # ### SCALE SEQUENCES ###

    # scaler_seq = StandardScaler()
    # scaler_feat = StandardScaler()

    # X_train_feat = scaler_feat.fit_transform(X_train_feat.reshape(-1,ns)).reshape(X_train_feat.shape)
    # X_test_feat = scaler_feat.transform(X_test_feat.reshape(-1,ns)).reshape(X_test_feat.shape)

    # ### OBTAIN LAPLACIANS FROM CORRELATIONS ###

    # X_train_lap = localpooling_filter(1 - np.abs(X_train_cor))
    # X_test_lap = localpooling_filter(1 - np.abs(X_test_cor))

    # stock_cor_train_data = train_matrix_to_tfdata(X_train_lap)
    # stock_cor_test_data = test_matrix_to_tfdata(X_test_lap)

    # stock_feat_train_data = train_matrix_to_tfdatafeat(X_train_feat)
    # stock_feat_test_data = test_matrix_to_tfdatafeat(X_test_feat)
    # #################################################################################################################

    # stock_lstm_train_data = tf.data.Dataset.from_generator(
    #     lambda: train_generator,
    #     output_types=(tf.float32, tf.float32),
    #     output_shapes=([batch_size, time_steps, feature_traindata.shape[1]], [batch_size,]) # based on batch_size change
    #   )

    # stock_lstm_test_data = tf.data.Dataset.from_generator(
    #     lambda: test_generator,
    #     output_types=(tf.float32, tf.float32),
    #     output_shapes=([batch_size, 1, feature_testdata.shape[1]], [batch_size,]) # based on batch_size change
    #   )

    #   # call above function
    # train_InputExamples = convert_train_to_examples(train, DATA_COLUMN, LABEL_COLUMN)
    #   # call above function
    # test_InputExamples = convert_test_to_examples(test, DATA_COLUMN, LABEL_COLUMN)

    # stock_train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)
    # stock_train_data = stock_train_data.batch(batch_size, drop_remainder=True)

    # stock_test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)
    # stock_test_data = stock_test_data.batch(batch_size, drop_remainder=True)

    #   # price test batch drop remainder
    # stock_lstm_test_data = stock_lstm_test_data.unbatch()
    # stock_lstm_test_data = stock_lstm_test_data.batch(batch_size, drop_remainder=True)


    #   # merged train data
    # try_data = tf.data.Dataset.zip((stock_lstm_train_data, stock_feat_train_data, stock_cor_train_data, stock_train_data))
    # try_data.element_spec
    # train_data = try_data.map(lambda x, x1, x2, y: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0), y[0], y[1], y[2]), x[1]))
    # print(train_data.element_spec)

    #   # merged validation data
    # try_data2 = tf.data.Dataset.zip((stock_lstm_test_data, stock_feat_test_data, stock_cor_test_data, stock_test_data))
    # try_data2.element_spec
    # test_data = try_data2.map(lambda x, x1, x2, y: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0), y[0], y[1], y[2]), x[1]))
    # print(test_data.element_spec)

    """# Tuning test"""

    # !pip install keras-tuner --upgrade

    # import keras_tuner
    # from tensorflow import keras
    # from keras import backend as K
    # from tensorflow.keras import layers, losses
    # import numpy as np
    # import matplotlib.pyplot as plt
    # import os

    # class MyHyperModel(keras_tuner.HyperModel) :
    #     def build(self, hp, classes=1) :
    #         # define model
    #         lstm_input = tf.keras.layers.Input(shape=(None, Multi_data.shape[1]-4), dtype=tf.float32, name="Price")

    #         gcn_cor_input = tf.keras.layers.Input(shape=(inp_lap.shape[1],inp_lap.shape[2]), dtype=tf.float32, name="cor")
    #         gcn_sf_input = tf.keras.layers.Input(shape=(inp_feat.shape[1],inp_feat.shape[2]), dtype=tf.float32, name="stat_feat")


    #         lstm_side = lstm_model(lstm_input)

    #         gcn_side = gcn_model([gcn_cor_input, gcn_sf_input])

    #         # Concatenate features from LSTM and GCN only (without BERT)
    #         merged = tf.keras.layers.Concatenate()([lstm_side, gcn_side])

    #         # Number of Layers is up to tuning
    #         for i in range(hp.Int("num_layer", min_value=2, max_value=8, step=2)) :
    #             # Tune hyperparams of each conv layer separately by using f"...{i}"

    #             merged = (layers.Normalization())(merged)

    #             merged = (layers.Dense(hp.Int(name=f"Dense_{i}", min_value=40, max_value=160, step=20),
    #                                     activation=hp.Choice('layer_opt',['relu'])))(merged)


    #         # Last layer
    #         output = (layers.Dense(classes, activation='sigmoid'))(merged)

    #         model = models.Model([lstm_input, gcn_cor_input, gcn_sf_input], output)

    #         # Picking an opimizer and a loss function
    #         model.compile(optimizer=hp.Choice('optim',['adam']),
    #                       loss="binary_crossentropy",
    #                       metrics = ['accuracy'])

    #         # A way to optimize the learning rate while also trying different optimizers
    #         learning_rate = hp.Choice('lr', [0.0001])
    #         K.set_value(model.optimizer.learning_rate, learning_rate)

    #         return model


    #     def fit(self, hp, model,x, *args, **kwargs) :

    #         return model.fit( x,
    #                          *args,
    #                          shuffle=hp.Boolean("shuffle"),
    #                          **kwargs)

    # classes = 1
    # BATCH_SIZE = 1
    # hp = keras_tuner.HyperParameters()
    # hypermodel = MyHyperModel()
    # model = hypermodel.build(hp, classes)

    # tuner = keras_tuner.BayesianOptimization(
    #                         hypermodel=MyHyperModel(),
    #                         objective = "val_accuracy",
    #                         max_trials = tmax_trials, #max candidates to test
    #                         overwrite=True,
    #                         directory='BO_search_dir',
    #                         project_name='multimodal_lstm_gcn')

    # model.summary()

    # tuner.search(x=train_data,
    #              validation_data = test_data,
    #              epochs=tepochs,
    #              callbacks=[earlystopping])

    # tuner.results_summary(1)

    # tuner.get_best_hyperparameters(5)

    # # Get the top 2 hyperparameters.
    # best_hps = tuner.get_best_hyperparameters(1)
    # # Build the model with the best hp.
    # h_model = MyHyperModel()
    # merge_model = h_model.build(best_hps[0])

    merge_model.summary()

    # history = model.fit(x=train_data,
    #                     validation_data=test_data,
    #                     epochs=100,
    #                     callbacks=[earlystopping])

    """
    # For Uptrend or Downtrend"""

    Multi_data['symbol'] = Multi_data['symbol'].apply(str.upper)
    Multi_data = Multi_data[Multi_data['symbol'] == triker1]
    # rslt_df = dataframe[dataframe['Stream'].isin(options)]

    def startday(trend):
        if trend == 'downtrend':
            data = Multi_data[Multi_data['date']>= '2022-01-01']
        elif trend == 'uptrend':
            data = Multi_data[Multi_data['date']>= '2021-07-01']
            data = data[data['date']< '2022-01-01']
            # data = data[data['date']< '2021-07-09']
        return data

    data_2022 = startday(trend)
    data_2022

    def mulday(trend):
        if trend == 'downtrend':
            Full_data = Multi_data
        elif trend == 'uptrend':
            Full_data = Multi_data[Multi_data['date']< '2022-01-01']
        return Full_data

    Full_data = mulday(trend)
    Full_data

    data_tail205 = Full_data.tail(len(data_2022)+22)
    data_tail205

    lstm_col_names = data_tail205.columns[3:20]
    # Accumulative price from the beggining of the day and evaluate to traning set
    li = []
    window_sizes = []
    importance_scores = []
    for i in range(0,len(data_tail205)-22):
        # 首先定義時間步長
        time_steps = 20+i
        batch_size = 1
        
        # 檢查是否有足夠的數據創建時間序列
        required_data_length = 21 + i + 2  # 訓練數據 + 測試數據
        if required_data_length > len(data_tail205):
            print(f"迭代 {i}: 數據不足，需要 {required_data_length} 行，但只有 {len(data_tail205)} 行")
            continue
            
        # 檢查測試數據是否足夠創建時間序列
        test_start_idx = max(0, 21+i-time_steps)
        test_data_length = (23+i) - test_start_idx
        if test_data_length < time_steps + 1:  # 需要 time_steps + 1 才能創建序列
            print(f"迭代 {i}: 測試數據不足，需要至少 {time_steps + 1} 行，但只有 {test_data_length} 行")
            continue
        
        train = data_tail205.iloc[0:21+i]
        feature_traindata = train.iloc[:,3:20].to_numpy()
        label_train = train.Diff.values

        val = data_tail205.iloc[0:21+i]
        feature_valdata = val.iloc[:,3:20].to_numpy()
        label_val = val.Diff.values

        # 修正：測試數據需要包含足夠的歷史數據來創建時間序列
        # 取從訓練結束前 time_steps 天開始到預測目標天的數據
        test = data_tail205.iloc[test_start_idx:23+i]  
        feature_testdata = test.iloc[:,3:20].to_numpy()
        label_test = test.Diff.values

        train_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
            data = feature_traindata,
            targets = label_train,
            length = time_steps,
            sampling_rate=1,
            stride=1,
            start_index=0,
            end_index=None,
            shuffle=False,
            reverse=False,
            batch_size=batch_size
        )

        val_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
            data = feature_valdata,
            targets = label_val,
            length = time_steps,
            sampling_rate=1,
            stride=1,
            start_index=0,
            end_index=None,
            shuffle=False,
            reverse=False,
            batch_size=batch_size
        )

        test_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(
            data = feature_testdata,
            targets = label_test,
            length = time_steps,
            sampling_rate=1,
            stride=1,
            start_index=0,
            end_index=len(feature_testdata)-1,  # 確保只預測最後一天
            shuffle=False,
            reverse=False,
            batch_size=batch_size
        )

        ###############################################################################################################
        ### CREATE TRAIN FEATURES ###
        # For LSTM

        # train_date = dc(train.head(1)['date'])

        # test_date = dc(test.head(1)['date'])

        # final_date = dc(test.tail(1)['date'])

        train_head = train.head(1)['date'].to_string().split()[-1]
        train_tail = train.tail(1)['date'].to_string().split()[-1]
        test_head = test.head(1)['date'].to_string().split()[-1]
        test_tail = test.tail(1)['date'].to_string().split()[-1]

        X_seq, X_cor, X_feat = [], [], []

        for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(train_head):unstaked_df.columns.get_loc(train_tail)]:
            corr_, feat_ = create_features(unstaked_df, d)
            X_cor.append(corr_), X_feat.append(feat_)

        X_train_cor = np.concatenate(X_cor, axis=0).astype('float16')
        X_train_feat = np.concatenate(X_feat, axis=0).astype('float16')

        print(X_train_cor.shape, X_train_feat.shape)
        #val
        valtrain_today = training.head(1)['date'].to_string().split()[-1]
        valtest_today = testing.head(1)['date'].to_string().split()[-1]

        X_seq, X_cor, X_feat = [], [], []

        for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(train_head):unstaked_df.columns.get_loc(train_tail)]:
            corr_, feat_ = create_features(unstaked_df, d)
            X_cor.append(corr_), X_feat.append(feat_)

        X_val_cor = np.concatenate(X_cor, axis=0).astype('float16')
        X_val_feat = np.concatenate(X_feat, axis=0).astype('float16')

        print(X_val_cor.shape, X_val_feat.shape)

        # ## CREATE TEST FEATURES ###

        X_seq, X_cor, X_feat = [], [], []

        for d in unstaked_df.iloc[:, unstaked_df.columns.get_loc(test_head):unstaked_df.columns.get_loc(test_tail)+1]:
            corr_, feat_ = create_features(unstaked_df, d)
            X_cor.append(corr_), X_feat.append(feat_)

        X_test_cor = np.concatenate(X_cor, axis=0).astype('float16')
        X_test_feat = np.concatenate(X_feat, axis=0).astype('float16')

        print(X_test_cor.shape, X_test_feat.shape)

        ### SCALE SEQUENCES ###

        scaler_seq = StandardScaler()
        scaler_feat = StandardScaler()

        X_train_feat = scaler_feat.fit_transform(X_train_feat.reshape(-1,ns)).reshape(X_train_feat.shape)
        X_val_feat = scaler_feat.fit_transform(X_val_feat.reshape(-1,ns)).reshape(X_val_feat.shape)
        X_test_feat = scaler_feat.transform(X_test_feat.reshape(-1,ns)).reshape(X_test_feat.shape)

        ### OBTAIN LAPLACIANS FROM CORRELATIONS ###

        X_train_lap = localpooling_filter(1 - np.abs(X_train_cor))
        X_val_lap = localpooling_filter(1 - np.abs(X_val_cor))
        X_test_lap = localpooling_filter(1 - np.abs(X_test_cor))

        stock_cor_train_data = train_matrix_to_tfdata(X_train_lap)
        stock_cor_val_data = train_matrix_to_tfdata(X_val_lap)
        stock_cor_test_data = test_matrix_to_tfdata(X_test_lap)

        stock_feat_train_data = train_matrix_to_tfdatafeat(X_train_feat)
        stock_feat_val_data = train_matrix_to_tfdatafeat(X_val_feat)
        stock_feat_test_data = test_matrix_to_tfdatafeat(X_test_feat)
        #################################################################################################################

        stock_lstm_train_data = tf.data.Dataset.from_generator(
            lambda: train_generator,
            output_types=(tf.float32, tf.float32),
            output_shapes=([batch_size, time_steps, feature_traindata.shape[1]], [batch_size,]) # based on batch_size change
            )

        stock_lstm_val_data = tf.data.Dataset.from_generator(
            lambda: val_generator,
            output_types=(tf.float32, tf.float32),
            output_shapes=([batch_size, time_steps, feature_valdata.shape[1]], [batch_size,]) # based on batch_size change
        )

        stock_lstm_test_data = tf.data.Dataset.from_generator(
            lambda: test_generator,
            output_types=(tf.float32, tf.float32),
            output_shapes=([batch_size, time_steps, feature_testdata.shape[1]], [batch_size,]) # based on batch_size change
            )

            # price test batch drop remainder
        stock_lstm_test_data = stock_lstm_test_data.unbatch()
        stock_lstm_test_data = stock_lstm_test_data.batch(batch_size, drop_remainder=True)


            # merged train data without BERT
        try_data = tf.data.Dataset.zip((stock_lstm_train_data, stock_feat_train_data, stock_cor_train_data))
        try_data.element_spec
        train_data = try_data.map(lambda x, x1, x2: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0)), x[1]))
        print(train_data.element_spec)

        try_valdata = tf.data.Dataset.zip((stock_lstm_val_data, stock_feat_val_data, stock_cor_val_data))
        try_valdata.element_spec
        val_data = try_valdata.map(lambda x, x1, x2: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0)), x[1]))
        print(val_data.element_spec)

            # merged validation data without BERT
        try_data2 = tf.data.Dataset.zip((stock_lstm_test_data, stock_feat_test_data, stock_cor_test_data))
        try_data2.element_spec
        test_data = try_data2.map(lambda x, x1, x2: ((x[0], tf.expand_dims(x2[0], axis=0), tf.expand_dims(x1[0], axis=0)), x[1]))
        print(test_data.element_spec)

        # Train the model
        earlyfs_history = merge_model.fit(train_data,
                                            epochs=fepochs,
                                            callbacks=[earlystopping],
                                            validation_data = val_data
                                            # verbose=1
        )

        eval_results = merge_model.predict(test_data)
        print(eval_results)
        li.append(eval_results)
        print(len(data_tail205)-22-i)

        # --- 把結果取出 ---
        val_loss = earlyfs_history.history['val_loss'][-1]   # 取最後一個 epoch 的 validation loss
        val_acc = earlyfs_history.history.get('val_accuracy', [None])[-1]  # 如果有 accuracy

        # 記錄 window size 與對應的分數
        time_steps = 20 + i
        window_sizes.append(time_steps)

        # importance 可以是 1 - val_loss，或直接取 val_acc
        if val_acc is not None:
                # importance_scores.append(val_acc)
                importance_scores.append(1 - val_loss)
        else:
                importance_scores.append(1 - val_loss)

    li = np.array(li)
    y_pred = np.where(li > 0.5, 1, 0)

    true_y = data_tail205.tail(len(y_pred)).Diff.values
    true_y

    y_PRED = []
    for i in range(0, len(li)):
        k = y_pred[i][0][0]
        y_PRED.append(k)
    y_PRED = np.array(y_PRED)
    y_PRED

    from sklearn.metrics import confusion_matrix
    confusion_matrix(true_y, y_PRED)

    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc

    fpr, tpr, thresholds = roc_curve(true_y, y_PRED)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
    # plt.xlim([0.0, 1.0])
    # plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate or (1 - Specifity)')
    plt.ylabel('True Positive Rate or (Sensitivity)')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")

    from sklearn.metrics import classification_report
    print('Results on the test set:')
    cr = classification_report(true_y, y_PRED, output_dict=True)
    print(cr)

    acc = str(round(cr['accuracy']*100, 2))
    acc

    crdf = pd.DataFrame(cr).transpose()
    crdf

    # crdf.to_csv(path + 'classreport_'+triker + '_' + trend +'_' + acc + '.csv', index=False)

    conf_matrix = confusion_matrix(true_y, y_PRED)

    fig, ax = plt.subplots(figsize=(7.5, 7.5))
    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)
    for i in range(conf_matrix.shape[0]):
        for j in range(conf_matrix.shape[1]):
            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

    plt.xlabel('Predictions', fontsize=18)
    plt.ylabel('Actuals', fontsize=18)
    plt.title('Confusion Matrix', fontsize=18)
    # plt.savefig(path + triker + '_' + trend +'_' + acc + 'confusionmatrix.png')
    plt.show()

    obs = np.arange(0,len(y_PRED))
    len(obs)

    date_2 = data_2022['date']
    len(date_2)

    data = {'': obs,
            'y_PRED': y_PRED,
            'ground_truth': true_y,
            'dates': date_2}

    df = pd.DataFrame(data)
    df

    print(path + triker + '_' + trend +'_' + acc + '_GCN_Multi.csv')

    # df.to_csv(path + triker1 + '_' + trend +'_' + acc + '_GCN_LSTMulti.csv', index=False)
    # df.to_csv('/content/drive/MyDrive/stock_results_tuned/GCN_LSTMulti/' + triker1 + '_' + trend +'_' + acc + '_GCN_LSTMulti.csv', index=False)
    df.to_csv(path + triker + '_' + trend +'_' + acc + '_GCN_Multi.csv', index=False)
    df.to_csv(path + triker + '_' + trend + '_GCN_Multi.csv', index=False)

    """# Load model"""

    # model_structure = merge_model.to_json()
    # with open(path + triker + '_' + trend +'_' + acc + '.json', 'w') as json_file:
    #     json_file.write(model_structure)
    # merge_model.save_weights(path + triker + '_' + trend +'_' + acc + '.h5')
    # print('Model saved.')

    # from keras.models import model_from_json
    # with open(path + triker + '_' + trend +'_' + acc + '.json', 'r') as json_file:
    #     json_string = json_file.read()
    # model = model_from_json(json_string)

    # model.load_weights(path + triker + '_' + trend +'_' + acc + '.h5')

    """# feature importance"""

def lstm_feature_importance_saliency_v2(merge_model, lstm_input_data, feature_names, output_dir):
    """
    對 merge_model 的 LSTM 輸入計算梯度 × 輸入
    """
    import os
    os.makedirs(output_dir, exist_ok=True)
    print("Calculating LSTM feature importance (last 5 days)...")

    # 找到 merge_model 中的 LSTM 輸入層名稱
    lstm_input_layer_name = 'Price'  # 你的 LSTM 輸入層名稱
    lstm_input_tensor = merge_model.get_layer(lstm_input_layer_name).input

    last_5_data = lstm_input_data[-10:]

    for i, sample in enumerate(last_5_data):
        # 加 batch 維度
        x = tf.convert_to_tensor(sample[np.newaxis, ...], dtype=tf.float32)
        x = tf.Variable(x)

        with tf.GradientTape() as tape:
            tape.watch(x)
            # 將整個 merge_model 的輸出對 LSTM 輸入求梯度
            pred = merge_model([x] + [tf.zeros((1,) + l.shape[1:]) for l in merge_model.inputs[1:]])
        grads = tape.gradient(pred, x)
        importance = np.abs(grads.numpy()[0]).mean(axis=0)  # 對時間軸求平均

        # 畫圖
        plt.figure(figsize=(10, 4))
        plt.bar(feature_names, importance)
        plt.xticks(rotation=90)
        plt.ylabel("Importance (mean abs gradient)")
        plt.title(f"LSTM Feature Importance - Day {i+1}")
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"lstm_feature_importance_day_{i+1}.png")
        plt.savefig(plot_path)
        plt.close()
        print(f"Saved plot: {plot_path}")

import seaborn as sns

def gcn_correlation_heatmap(correlation_matrices, tickers, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    print("Generating GCN correlation heatmaps (last 5 days)...")

    for i, corr in enumerate(correlation_matrices[-5:]):
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr, xticklabels=tickers, yticklabels=tickers, cmap='viridis')
        plt.title(f'GCN Correlation Matrix - Day {i+1}')
        plot_path = os.path.join(output_dir, f'gcn_correlation_day_{i+1}.png')
        plt.savefig(plot_path)
        plt.close()
        print(f"Saved plot: {plot_path}")


# 簡化的 LSTM 特徵重要性函數
def simple_lstm_feature_analysis(lstm_input, feature_names, output_dir, max_days=10):
    """
    簡化版的 LSTM 特徵分析 - 分析每一天的特徵重要性
    lstm_input: shape (samples, timesteps, features) 或 (samples, features)
    max_days: 分析的最大天數
    """
    os.makedirs(output_dir, exist_ok=True)
    print(f"Performing simplified LSTM feature analysis for first {max_days} days...")
    
    if len(lstm_input.shape) == 3:  # (samples, timesteps, features)
        num_samples = min(lstm_input.shape[0], max_days)
        print(f"Analyzing {num_samples} days out of {lstm_input.shape[0]} available samples")
        
        # 分析每一天的特徵重要性
        for day in range(num_samples):
            # 取第 day 個樣本的數據
            day_data = lstm_input[day]  # shape: (timesteps, features)
            
            # 計算這一天的特徵重要性（對時間軸求平均）
            if len(day_data.shape) == 2:  # (timesteps, features)
                feature_importance = np.mean(np.abs(day_data), axis=0)  # 對時間軸求平均
            else:  # (features,)
                feature_importance = np.abs(day_data)
            
            # 為每一天創建圖表
            plt.figure(figsize=(14, 6))
            bars = plt.bar(range(len(feature_names)), feature_importance, alpha=0.7, color='steelblue')
            plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')
            plt.ylabel("Feature Magnitude (mean abs value)")
            plt.title(f"LSTM Feature Importance - Day {day + 1}")
            plt.grid(True, alpha=0.3)
            
            # 添加數值標籤到較高的柱子上
            max_height = max(feature_importance) if len(feature_importance) > 0 else 1
            for i, (bar, val) in enumerate(zip(bars, feature_importance)):
                if val > max_height * 0.1:  # 只顯示較大值的標籤
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_height*0.01, 
                            f'{val:.3f}', ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            plot_path = os.path.join(output_dir, f"lstm_feature_analysis_day_{day + 1}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"Saved Day {day + 1} analysis plot: {plot_path}")
            
        # 創建一個總結圖：所有天的平均特徵重要性
        overall_importance = np.mean(np.abs(lstm_input[:num_samples]), axis=(0, 1))
        
        plt.figure(figsize=(14, 6))
        bars = plt.bar(range(len(feature_names)), overall_importance, alpha=0.7, color='darkgreen')
        plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')
        plt.ylabel("Feature Magnitude (mean abs value)")
        plt.title(f"LSTM Feature Importance - Overall Average (First {num_samples} Days)")
        plt.grid(True, alpha=0.3)
        
        # 添加數值標籤
        max_height = max(overall_importance) if len(overall_importance) > 0 else 1
        for i, (bar, val) in enumerate(zip(bars, overall_importance)):
            if val > max_height * 0.1:
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_height*0.01, 
                        f'{val:.3f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"lstm_feature_analysis_overall_avg_{num_samples}days.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Saved overall average analysis plot: {plot_path}")
        
    elif len(lstm_input.shape) == 2:  # (samples, features) - 沒有時間維度
        num_samples = min(lstm_input.shape[0], max_days)
        print(f"Analyzing {num_samples} days out of {lstm_input.shape[0]} available samples (no time dimension)")
        
        # 分析每一天的特徵重要性
        for day in range(num_samples):
            feature_importance = np.abs(lstm_input[day])  # 第 day 個樣本的特徵
            
            plt.figure(figsize=(14, 6))
            bars = plt.bar(range(len(feature_names)), feature_importance, alpha=0.7, color='steelblue')
            plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')
            plt.ylabel("Feature Magnitude (abs value)")
            plt.title(f"LSTM Feature Importance - Day {day + 1}")
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plot_path = os.path.join(output_dir, f"lstm_feature_analysis_day_{day + 1}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"Saved Day {day + 1} analysis plot: {plot_path}")
            
    else:
        print(f"Unexpected LSTM input shape: {lstm_input.shape}")


# 輸出資料夾
output_dir = "feature_importance_output"
os.makedirs(output_dir, exist_ok=True)

# LSTM 輸入資料 (最後 reshape 成 [samples, timesteps, features])
lstm_input = feature_testdata.reshape(-1, 1, feature_testdata.shape[1])  # 根據你的 pipeline 調整
feature_names = lstm_col_names.tolist()  # 你原本的 LSTM feature names

# 計算 LSTM 特徵重要性 - 使用簡化版本
try:
    simple_lstm_feature_analysis(lstm_input, feature_names, output_dir)
except Exception as e:
    print(f"Error in LSTM feature analysis: {e}")

# GCN correlation matrices
tickers = lstm_col_names.tolist()
gcn_correlation_heatmap(X_test_cor, tickers, output_dir)

def lstm_feature_importance_saliency(merge_model, stock_lstm_train_data, feature_names, output_dir, last_n_days=3):
    """
    使用梯度計算 LSTM feature importance - 修復版本
    stock_lstm_train_data: tf.data.Dataset, 每個 batch shape=(batch_size, timesteps, features)
    feature_names: list of feature names
    last_n_days: int, 分析最後幾個 batch (天)
    """
    os.makedirs(output_dir, exist_ok=True)
    print(f"Calculating LSTM feature importance (last {last_n_days} days)...")

    # 收集訓練數據樣本
    lstm_samples = []
    gcn_cor_samples = []
    gcn_sf_samples = []
    word_samples = []
    mask_samples = []
    segment_samples = []
    
    batch_count = 0
    for batch_data in stock_lstm_train_data.take(last_n_days):
        try:
            # 打印實際的數據結構以便調試
            print(f"Batch data type: {type(batch_data)}")
            
            if isinstance(batch_data, tuple) and len(batch_data) == 2:
                inputs, targets = batch_data
                print(f"Inputs type: {type(inputs)}, Targets type: {type(targets)}")
                
                # 檢查 inputs 是否是 tuple/list 且包含多個元素
                if isinstance(inputs, (list, tuple)) and len(inputs) >= 6:
                    lstm_samples.append(inputs[0].numpy())
                    gcn_cor_samples.append(inputs[1].numpy())
                    gcn_sf_samples.append(inputs[2].numpy())
                    word_samples.append(inputs[3].numpy())
                    mask_samples.append(inputs[4].numpy())
                    segment_samples.append(inputs[5].numpy())
                    batch_count += 1
                    print(f"Batch {batch_count}: LSTM shape {inputs[0].shape}, targets shape {targets.shape}")
                elif hasattr(inputs, 'numpy'):
                    # 如果 inputs 是單個 tensor，可能是簡化的數據格式
                    print(f"Single tensor input with shape: {inputs.shape}")
                    lstm_samples.append(inputs.numpy())
                    batch_count += 1
                else:
                    print(f"Warning: Unexpected input format: {type(inputs)}")
                    if hasattr(inputs, '__len__'):
                        print(f"Input length: {len(inputs)}")
                    if hasattr(inputs, 'shape'):
                        print(f"Input shape: {inputs.shape}")
                    break
            else:
                print(f"Warning: Unexpected batch format: {type(batch_data)}")
                if isinstance(batch_data, tuple):
                    print(f"Batch data length: {len(batch_data)}")
                break
                
        except Exception as e:
            print(f"Error processing batch {batch_count + 1}: {e}")
            break
    
    if batch_count == 0:
        print("Error: No valid data samples collected")
        return
    
    print(f"Collected {batch_count} batches of data")
    
    # 只處理 LSTM 數據（如果只有 LSTM 數據可用）
    if len(lstm_samples) > 0:
        lstm_input_last_n = np.concatenate(lstm_samples, axis=0)
        print("Shape of LSTM input for saliency:", lstm_input_last_n.shape)
        print("Data range - LSTM input: min={:.6f}, max={:.6f}, mean={:.6f}".format(
            lstm_input_last_n.min(), lstm_input_last_n.max(), lstm_input_last_n.mean()))
        
        # 簡化版本：只分析 LSTM 特徵重要性
        for day in range(min(last_n_days, batch_count)):
            sample_idx = day
            if sample_idx >= lstm_input_last_n.shape[0]:
                continue
                
            print(f"\n=== Day {day+1} Analysis (Sample {sample_idx+1}) ===")
            
            # 準備 LSTM 輸入
            lstm_x = tf.Variable(lstm_input_last_n[sample_idx:sample_idx+1], dtype=tf.float32)
            print(f"LSTM input shape: {lstm_x.shape}")
            
            # 簡化版本：假設我們有一個只接受 LSTM 輸入的子模型
            # 由於完整的模型需要多個輸入，我們創建一個簡化的分析
            
            try:
                with tf.GradientTape() as tape:
                    tape.watch(lstm_x)
                    # 這裡需要根據實際模型結構調整
                    # 暫時跳過梯度計算，只顯示特徵統計
                    pass
                
                # 計算特徵的統計重要性
                feature_importance = np.abs(lstm_x.numpy()[0]).mean(axis=0)
                
                # 畫圖
                plt.figure(figsize=(12, 6))
                plt.bar(range(len(feature_names)), feature_importance)
                plt.xticks(range(len(feature_names)), feature_names, rotation=45)
                plt.ylabel("Feature Magnitude (mean abs value)")
                plt.title(f"LSTM Feature Analysis - Day {day+1}")
                plt.tight_layout()
                plot_path = os.path.join(output_dir, f"lstm_feature_analysis_day_{day+1}.png")
                plt.savefig(plot_path)
                plt.close()
                print(f"Saved plot: {plot_path}")
                
            except Exception as e:
                print(f"Error in gradient calculation for day {day+1}: {e}")
                continue
    else:
        print("No LSTM samples collected for analysis")
        return  # 如果沒有數據，直接返回
    
    print(f"Collected {batch_count} batches of data")
    
    # 檢查是否有足夠的數據進行分析
    if len(lstm_samples) == 0:
        print("No LSTM samples to analyze")
        return
    
    # 轉換為 numpy arrays
    try:
        lstm_input_last_n = np.concatenate(lstm_samples, axis=0)
        print("Shape of LSTM input for saliency:", lstm_input_last_n.shape)
        
        # 檢查是否有其他模態的數據
        if len(gcn_cor_samples) > 0:
            gcn_cor_last_n = np.concatenate(gcn_cor_samples, axis=0)
            print("Shape of GCN cor input:", gcn_cor_last_n.shape)
        
        if len(gcn_sf_samples) > 0:
            gcn_sf_last_n = np.concatenate(gcn_sf_samples, axis=0)
            print("Shape of GCN sf input:", gcn_sf_last_n.shape)
            
        if len(word_samples) > 0:
            word_ids_last_n = np.concatenate(word_samples, axis=0)
            mask_last_n = np.concatenate(mask_samples, axis=0)
            segment_last_n = np.concatenate(segment_samples, axis=0)
            
    except Exception as e:
        print(f"Error concatenating samples: {e}")
        return
    print("Shape of GCN sf input:", gcn_sf_last_n.shape)
    print("Data range - LSTM input: min={:.6f}, max={:.6f}, mean={:.6f}".format(
        lstm_input_last_n.min(), lstm_input_last_n.max(), lstm_input_last_n.mean()))

    # 分析每個天的數據（基於batch）
    current_idx = 0
    for day in range(min(last_n_days, batch_count)):
        # 獲取這一天的batch大小
        day_batch_size = lstm_samples[day].shape[0]
        
        # 使用這一天第一個樣本進行分析
        sample_idx = current_idx
        if sample_idx >= lstm_input_last_n.shape[0]:
            print(f"Day {day+1}: Sample index {sample_idx} out of range")
            break
            
        print(f"\n=== Day {day+1} Analysis (Sample {sample_idx+1}) ===")
        
        # 準備單個樣本的所有輸入
        lstm_x = tf.Variable(lstm_input_last_n[sample_idx:sample_idx+1], dtype=tf.float32)
        gcn_cor_x = tf.constant(gcn_cor_last_n[sample_idx:sample_idx+1], dtype=tf.float32)
        gcn_sf_x = tf.constant(gcn_sf_last_n[sample_idx:sample_idx+1], dtype=tf.float32)
        word_x = tf.constant(word_ids_last_n[sample_idx:sample_idx+1], dtype=tf.int32)
        mask_x = tf.constant(mask_last_n[sample_idx:sample_idx+1], dtype=tf.int32)
        segment_x = tf.constant(segment_last_n[sample_idx:sample_idx+1], dtype=tf.int32)

        print(f"Input shapes: LSTM {lstm_x.shape}, GCN_cor {gcn_cor_x.shape}, GCN_sf {gcn_sf_x.shape}")
        
        with tf.GradientTape() as tape:
            tape.watch(lstm_x)
            # 使用所有正確的輸入進行預測
            pred = merge_model([lstm_x, gcn_cor_x, gcn_sf_x, word_x, mask_x, segment_x])
            print(f"Prediction shape: {pred.shape}, values: {pred.numpy()}")
            
            # 修復：使用平均預測值而不是最大值，更穩定的梯度
            if len(pred.shape) > 1 and pred.shape[1] > 1:
                # 多類分類：使用所有類別的平均值
                target_score = tf.reduce_mean(pred)
            else:
                # 單值輸出
                target_score = tf.reduce_mean(pred)
                
        grads = tape.gradient(target_score, lstm_x)
        
        # 檢查梯度是否計算成功
        if grads is None:
            print(f"Warning: Gradient is None for day {day+1}")
            current_idx += day_batch_size
            continue
            
        print(f"Gradient shape: {grads.shape}")
        print(f"Gradient range: min={grads.numpy().min():.8f}, max={grads.numpy().max():.8f}, mean={grads.numpy().mean():.8f}")
        
        # 修復特徵重要性計算方法
        # 方法1: 使用梯度的絕對值平均（更穩定）
        importance_grad = np.abs(grads.numpy()[0]).mean(axis=0)
        
        # 方法2: 使用梯度×輸入的絕對值（經典saliency）
        saliency = grads * lstm_x
        importance_saliency = np.abs(saliency.numpy()[0]).mean(axis=0)
        
        # 方法3: 使用梯度平方（避免正負抵消）
        importance_grad_sq = np.square(grads.numpy()[0]).mean(axis=0)
        
        print(f"Day {day+1} - Gradient importance: min={importance_grad.min():.6f}, max={importance_grad.max():.6f}, mean={importance_grad.mean():.6f}")
        print(f"Day {day+1} - Saliency importance: min={importance_saliency.min():.6f}, max={importance_saliency.max():.6f}, mean={importance_saliency.mean():.6f}")
        print(f"Day {day+1} - Gradient squared: min={importance_grad_sq.min():.6f}, max={importance_grad_sq.max():.6f}, mean={importance_grad_sq.mean():.6f}")
        
        # 選擇最有效的方法
        if importance_saliency.max() > 0:
            importance = importance_saliency
            method_name = "Saliency (grad × input)"
        elif importance_grad.max() > 0:
            importance = importance_grad
            method_name = "Gradient magnitude"
        else:
            importance = importance_grad_sq
            method_name = "Gradient squared"
            
        if np.all(importance == 0):
            print(f"Warning: All importance values are zero for day {day+1}")
            current_idx += day_batch_size
            continue

        # 檢查特徵名稱和重要性長度匹配
        print(f"Feature names length: {len(feature_names)}, Importance length: {len(importance)}")
        
        if len(feature_names) != len(importance):
            print(f"Warning: Feature names ({len(feature_names)}) and importance ({len(importance)}) length mismatch")
            min_len = min(len(feature_names), len(importance))
            feature_names_plot = feature_names[:min_len]
            importance_plot = importance[:min_len]
        else:
            feature_names_plot = feature_names
            importance_plot = importance

        # 畫圖
        plt.figure(figsize=(12, 6))
        bars = plt.bar(feature_names_plot, importance_plot, alpha=0.8, edgecolor='black', linewidth=0.5)
        
        # 添加數值標籤
        for bar, value in zip(bars, importance_plot):
            height = bar.get_height()
            if height > 0:
                plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{value:.4f}', ha='center', va='bottom', fontsize=8)
        
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.ylabel(f"Importance ({method_name})", fontsize=10)
        plt.title(f"LSTM Feature Importance - Day {day+1}", fontsize=12, fontweight='bold')
        
        # 設置y軸範圍
        if importance_plot.max() > 0:
            plt.ylim(0, importance_plot.max() * 1.1)
        
        # 添加網格線
        plt.grid(axis='y', alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"lstm_feature_importance_day_{day+1}.png")
        
        try:
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"Saved plot: {plot_path} (importance range: {importance_plot.min():.6f} to {importance_plot.max():.6f})")
        except Exception as e:
            print(f"Error saving plot: {e}")
        finally:
            plt.close()
            
        current_idx += day_batch_size

    print("LSTM feature importance analysis completed.")


def gcn_correlation_heatmap(correlation_matrices, tickers, output_dir, last_n_days=5):
    os.makedirs(output_dir, exist_ok=True)
    print(f"Generating GCN correlation heatmaps (last {last_n_days} days)...")

    for i, corr in enumerate(correlation_matrices[-last_n_days:]):
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr, xticklabels=tickers, yticklabels=tickers, cmap='viridis')
        plt.title(f'GCN Correlation Matrix - Day {i+1}')
        plot_path = os.path.join(output_dir, f'gcn_correlation_day_{i+1}.png')
        plt.savefig(plot_path)
        plt.close()
        print(f"Saved plot: {plot_path}")


def gcn_correlationfeat_heatmap(correlation_matrices, tickers, tickers1, output_dir, last_n_days=5):
    os.makedirs(output_dir, exist_ok=True)
    print(f"Generating GCN correlation feat heatmaps (last {last_n_days} days)...")

    for i, corr in enumerate(correlation_matrices[-last_n_days:]):
        plt.figure(figsize=(12, 10))  # 增大圖片尺寸
        sns.heatmap(corr, xticklabels=tickers, yticklabels=tickers1, cmap='viridis')
        plt.title(f'GCN Correlation feat Matrix - Day {i+1}')
        
        # 調整 x 軸標籤的顯示
        plt.xticks(rotation=45, ha='right', fontsize=8)  # 旋轉角度並調整字體大小
        plt.yticks(rotation=0, fontsize=8)
        
        # 調整佈局以避免標籤被截斷
        plt.tight_layout()
        
        plot_path = os.path.join(output_dir, f'gcn_correlation_feat_day_{i+1}.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')  # 使用 bbox_inches='tight' 確保所有內容都包含在內
        plt.close()
        print(f"Saved plot: {plot_path}")

def gcn_correlation_heatmap_top5(correlation_matrices, tickers, output_dir, ticker, last_n_days=5):
    """
    畫出每一天的股票相關性熱圖，並同時畫出與指定 ticker 最相關的前5名長條圖

    Args:
        correlation_matrices: list[np.ndarray], 每個元素為 correlation matrix
        tickers: list[str], 股票代號清單
        output_dir: str, 輸出資料夾
        ticker: str, 想分析的目標股票
        last_n_days: int, 要畫出最近幾天的結果
    """
    os.makedirs(output_dir, exist_ok=True)
    print(f"Generating GCN correlation heatmaps and top-5 bar charts for {ticker} (last {last_n_days} days)...")

    if ticker not in tickers:
        raise ValueError(f"Ticker '{ticker}' not found in tickers list.")

    ticker_idx = tickers.index(ticker)

    for i, corr in enumerate(correlation_matrices[-last_n_days:]):
        # ---- 熱圖 ----
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr, xticklabels=tickers, yticklabels=tickers, cmap='viridis', vmin=-1, vmax=1)
        plt.title(f'GCN Correlation Matrix - Day {i+1}')
        heatmap_path = os.path.join(output_dir, f'gcn_correlation_day_{i+1}.png')
        plt.savefig(heatmap_path)
        plt.close()
        print(f"Saved heatmap: {heatmap_path}")

        # ---- 提取與 ticker 最相關的前 5 名 ----
        corr_with_ticker = corr[ticker_idx]
        sorted_indices = np.argsort(-np.abs(corr_with_ticker))  # 依照絕對值由大到小
        top_indices = [idx for idx in sorted_indices if idx != ticker_idx][:5]  # 排除自己
        
        top_tickers = [tickers[idx] for idx in top_indices]
        top_values = corr_with_ticker[top_indices]

        # ---- 畫長條圖 ----
        plt.figure(figsize=(8, 5))
        sns.barplot(x=top_tickers, y=top_values, hue=top_tickers, palette='viridis', legend=False)
        plt.title(f'Top 5 correlations with {ticker} - Day {i+1}')
        plt.ylabel('Correlation')
        plt.xlabel('Stock')
        plt.ylim(-1, 1)
        bar_path = os.path.join(output_dir, f'top5_corr_{ticker}_day_{i+1}.png')
        plt.savefig(bar_path)
        plt.close()
        print(f"Saved bar chart: {bar_path}")
# ==================== 使用範例 ====================
output_dir = "feature_importance_output"
os.makedirs(output_dir, exist_ok=True)

# 計算 LSTM 特徵重要性，使用簡化版本避免複雜的數據格式問題
print('Performing simplified LSTM feature analysis...')
try:
    # 使用實際的特徵數據進行簡化分析 - 分析前10天
    if 'feature_testdata' in locals():
        simple_lstm_feature_analysis(feature_testdata.reshape(-1, 1, feature_testdata.shape[1]), 
                                    lstm_col_names.tolist(), output_dir, max_days=10)
    else:
        print("feature_testdata not available, skipping LSTM analysis")
except Exception as e:
    print(f"Error in LSTM feature analysis: {e}")

# 基於 LSTM 模型生成與特定特徵最相關的前5個特徵長條圖
# 查看可用的 LSTM 特徵名稱
print("Available LSTM features:", lstm_col_names.tolist())

# GCN correlation matrices
print(X_train_cor.shape)
# print(lstm_col_names.tolist())
stock_list = ['aapl', 'adbe', 'amt', 'amzn', 'atvi', 'bio', 'cvs', 'ebay', 'goog', 'ibm', 'jnj', 'jpm', 'msft', 'ndaq', 'nflx', 'nvda', 'pld', 'sbux', 'schw', 'tsla', 'vici']
gcn_correlation_heatmap(X_train_cor, stock_list, output_dir, last_n_days=5)

print(X_train_feat.shape)
feat_col_names = ['Full Sequence Mean', 'Mean Excluding First Period', 'Full Sequence Std', 'Std Excluding First Period', 'skewness', 'kurtosis', 'Linear Trend Slope']
# print(feat_col_names.tolist())
gcn_correlationfeat_heatmap(X_train_feat, feat_col_names, stock_list, output_dir, last_n_days=5)

gcn_correlation_heatmap_top5(X_train_cor, stock_list, "./output", ticker="aapl", last_n_days=5)

# 轉換為 NumPy 數組
window_sizes_np = np.array(window_sizes)
importance_scores_np = np.array(importance_scores)

# 創建布爾遮罩
mask = (window_sizes_np >= 120) & (window_sizes_np <= 140)

# 篩選數據
filtered_window_sizes = window_sizes_np[mask]
filtered_importance_scores = importance_scores_np[mask]

if len(filtered_window_sizes) > 0:
    plt.figure(figsize=(8, 5))
    plt.plot(filtered_window_sizes, filtered_importance_scores, 'o-r', linewidth=2, markersize=8)
    plt.title("Time Window Importance (120-140 days)")
    plt.xlabel("Window Size (days)")
    plt.ylabel("Importance Score")
    plt.grid(True)
    plt.xlim(120, 140)
    bar_path = os.path.join(output_dir, f'time_window_importance_120_140.png')
    plt.savefig(bar_path)
    plt.close()
else:
    print("No data in the 120-140 window size range")